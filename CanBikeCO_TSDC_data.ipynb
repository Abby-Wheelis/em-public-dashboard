{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3060cd",
   "metadata": {},
   "source": [
    "# TSDC Data Reconciling\n",
    "\n",
    "waiting on updates to the pueblo data (currently a copy of the smart commute data) and then \"unlocking\" redacted columns with the data researchers will need to apply for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1de9d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5f89fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading mapping dictionaries from mapping_dictionaries notebook\n",
    "%store -r df_ei\n",
    "%store -r dic_re\n",
    "%store -r dic_pur\n",
    "%store -r dic_fuel\n",
    "\n",
    "# convert a dictionary to a defaultdict\n",
    "dic_re = defaultdict(lambda: 'Other',dic_re)\n",
    "dic_pur = defaultdict(lambda: 'Other',dic_pur)\n",
    "dic_fuel = defaultdict(lambda: 'Other',dic_fuel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885782b8",
   "metadata": {},
   "source": [
    "## Mini Pilot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e35a999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_confirmed_trips = pd.read_csv('mini_pilot/data/analysis_confirmed_trip.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "23941706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3492\n",
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['_id', 'perno', 'metadata_platform', 'metadata_write_ts',\n",
       "       'metadata_time_zone', 'metadata_write_local_dt_year',\n",
       "       'metadata_write_local_dt_month', 'metadata_write_local_dt_day',\n",
       "       'metadata_write_local_dt_hour', 'metadata_write_local_dt_minute',\n",
       "       'metadata_write_local_dt_second', 'metadata_write_local_dt_weekday',\n",
       "       'metadata_write_local_dt_timezone', 'metadata_write_fmt_time',\n",
       "       'data_source', 'data_end_ts', 'data_end_local_dt_year',\n",
       "       'data_end_local_dt_month', 'data_end_local_dt_day',\n",
       "       'data_end_local_dt_hour', 'data_end_local_dt_minute',\n",
       "       'data_end_local_dt_second', 'data_end_local_dt_weekday',\n",
       "       'data_end_local_dt_timezone', 'data_end_fmt_time', 'data_end_loc_type',\n",
       "       'data_raw_trip', 'data_start_ts', 'data_start_local_dt_year',\n",
       "       'data_start_local_dt_month', 'data_start_local_dt_day',\n",
       "       'data_start_local_dt_hour', 'data_start_local_dt_minute',\n",
       "       'data_start_local_dt_second', 'data_start_local_dt_weekday',\n",
       "       'data_start_local_dt_timezone', 'data_start_fmt_time',\n",
       "       'data_start_loc_type', 'data_duration', 'data_distance',\n",
       "       'data_start_place', 'data_end_place', 'data_cleaned_trip',\n",
       "       'data_user_input_mode_confirm', 'data_user_input_purpose_confirm',\n",
       "       'data_user_input_replaced_mode', 'data_end_loc_longitude',\n",
       "       'data_end_loc_latitude', 'data_start_loc_longitude',\n",
       "       'data_start_loc_latitude', 'data_distance_miles', 'start_loc_geom_nrel',\n",
       "       'end_loc_geom_nrel', 'start_loc_geom_nrel_zipcode',\n",
       "       'end_loc_geom_nrel_zipcode'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(mini_confirmed_trips))\n",
    "print(mini_confirmed_trips.perno.nunique())\n",
    "mini_confirmed_trips.columns\n",
    "\n",
    "#this is one more user and about 1,000 more trips than we had in our minipilot dataset \n",
    "## - but we haven't removed no labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "6a9bbef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2403\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "## remove trips with no label and count again\n",
    "labeled_mini = mini_confirmed_trips[mini_confirmed_trips.data_user_input_mode_confirm.notna()]\n",
    "labeled_mini = mini_confirmed_trips[mini_confirmed_trips.data_user_input_purpose_confirm.notna()]\n",
    "# labeled_mini = mini_confirmed_trips[mini_confirmed_trips.data_user_input_purpose_confirm.notna()]\n",
    "\n",
    "print(len(labeled_mini)) #only 25 over data used in paper\n",
    "print(labeled_mini.perno.nunique())#same as data used in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "7f92c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data = labeled_mini.copy()\n",
    "\n",
    "#first, add the cleaned mode\n",
    "mini_data['Mode_confirm']= mini_data['data_user_input_mode_confirm'].map(dic_re)\n",
    "\n",
    "#second, add the cleaned replaced mode ASSUMES PROGRAM\n",
    "mini_data['Replaced_mode']= mini_data['data_user_input_replaced_mode'].map(dic_re)\n",
    "\n",
    "#third, add the cleaned purpose\n",
    "mini_data['Trip_purpose']= mini_data['data_user_input_purpose_confirm'].map(dic_pur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "133ca6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2354\n"
     ]
    }
   ],
   "source": [
    "# Combine variable categories\n",
    "mini_data = mini_data.replace('Gas Car, drove alone', 'Car')\n",
    "mini_data = mini_data.replace('Gas Car, with others', 'Shared Car')\n",
    "mini_data = mini_data.replace('Bikeshare', 'Shared Micromobility')\n",
    "mini_data = mini_data.replace('Scooter share', 'Shared Micromobility')\n",
    "mini_data = mini_data.replace('Regular Bike', 'Personal Micromobility')\n",
    "mini_data = mini_data.replace('Skate board', 'Personal Micromobility')\n",
    "mini_data = mini_data.replace('Train', 'Transit')\n",
    "mini_data = mini_data.replace('Free Shuttle', 'Transit')\n",
    "mini_data = mini_data.replace('Bus', 'Transit')\n",
    "mini_data = mini_data.replace('Walk', 'Walk')\n",
    "mini_data = mini_data.replace('Taxi/Uber/Lyft', 'Ridehail')\n",
    "mini_data = mini_data.replace('Pilot ebike', 'E-Bike')\n",
    "\n",
    "#filter out 'not a trip' trips\n",
    "mini_data = mini_data[~mini_data['Mode_confirm'].isin(['Not a Trip'])]\n",
    "mini_data = mini_data[~mini_data['Replaced_mode'].isin(['Not a Trip'])]\n",
    "mini_data = mini_data[~mini_data['Trip_purpose'].isin(['not_a_trip'])]\n",
    "\n",
    "print(len(mini_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "819b1dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data.loc[mini_data['Mode_confirm']=='Personal Micromobility', 'Mode_confirm'] = 'Other'\n",
    "mini_data.loc[mini_data['Mode_confirm']=='Shared Micromobility', 'Mode_confirm'] = 'Other'\n",
    "\n",
    "t1 = mini_data.groupby(['Mode_confirm'], as_index=False).count()[['Mode_confirm','data_distance']]\n",
    "t1['proportion'] = t1['data_distance'] / np.sum(t1.data_distance)\n",
    "t1['trip_type'] = 'All Trips'\n",
    "\n",
    "t2 = mini_data[mini_data['Trip_purpose']=='Work'].copy()\n",
    "t2 = t2.groupby(['Mode_confirm'], as_index=False).count()[['Mode_confirm','data_distance']]\n",
    "t2['proportion'] = t2['data_distance'] / np.sum(t2.data_distance)\n",
    "t2['trip_type'] = 'Work Trips'\n",
    "t2.loc[1.5] = 'Other', 0, 0, 'Work Trips'\n",
    "t2 = t2.sort_index().reset_index(drop=True)\n",
    "\n",
    "mini_data = pd.concat([t1,t2])\n",
    "mini_data['Dataset'] = 'Minipilot'\n",
    "mini_data.columns = ['Mode','Count','Proportion','Trip Type', \"Dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "465e51ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mode</th>\n",
       "      <th>Count</th>\n",
       "      <th>Proportion</th>\n",
       "      <th>Trip Type</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Car</td>\n",
       "      <td>477</td>\n",
       "      <td>0.202634</td>\n",
       "      <td>All Trips</td>\n",
       "      <td>Minipilot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E-bike</td>\n",
       "      <td>776</td>\n",
       "      <td>0.329652</td>\n",
       "      <td>All Trips</td>\n",
       "      <td>Minipilot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Other</td>\n",
       "      <td>28</td>\n",
       "      <td>0.011895</td>\n",
       "      <td>All Trips</td>\n",
       "      <td>Minipilot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridehail</td>\n",
       "      <td>65</td>\n",
       "      <td>0.027613</td>\n",
       "      <td>All Trips</td>\n",
       "      <td>Minipilot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shared Car</td>\n",
       "      <td>685</td>\n",
       "      <td>0.290994</td>\n",
       "      <td>All Trips</td>\n",
       "      <td>Minipilot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transit</td>\n",
       "      <td>155</td>\n",
       "      <td>0.065845</td>\n",
       "      <td>All Trips</td>\n",
       "      <td>Minipilot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Walk</td>\n",
       "      <td>168</td>\n",
       "      <td>0.071368</td>\n",
       "      <td>All Trips</td>\n",
       "      <td>Minipilot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Car</td>\n",
       "      <td>110</td>\n",
       "      <td>0.295699</td>\n",
       "      <td>Work Trips</td>\n",
       "      <td>Minipilot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E-bike</td>\n",
       "      <td>134</td>\n",
       "      <td>0.360215</td>\n",
       "      <td>Work Trips</td>\n",
       "      <td>Minipilot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Work Trips</td>\n",
       "      <td>Minipilot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridehail</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>Work Trips</td>\n",
       "      <td>Minipilot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shared Car</td>\n",
       "      <td>101</td>\n",
       "      <td>0.271505</td>\n",
       "      <td>Work Trips</td>\n",
       "      <td>Minipilot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>Work Trips</td>\n",
       "      <td>Minipilot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Walk</td>\n",
       "      <td>23</td>\n",
       "      <td>0.061828</td>\n",
       "      <td>Work Trips</td>\n",
       "      <td>Minipilot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Mode  Count  Proportion   Trip Type    Dataset\n",
       "0         Car    477    0.202634   All Trips  Minipilot\n",
       "1      E-bike    776    0.329652   All Trips  Minipilot\n",
       "2       Other     28    0.011895   All Trips  Minipilot\n",
       "3    Ridehail     65    0.027613   All Trips  Minipilot\n",
       "4  Shared Car    685    0.290994   All Trips  Minipilot\n",
       "5     Transit    155    0.065845   All Trips  Minipilot\n",
       "6        Walk    168    0.071368   All Trips  Minipilot\n",
       "0         Car    110    0.295699  Work Trips  Minipilot\n",
       "1      E-bike    134    0.360215  Work Trips  Minipilot\n",
       "2       Other      0    0.000000  Work Trips  Minipilot\n",
       "3    Ridehail      1    0.002688  Work Trips  Minipilot\n",
       "4  Shared Car    101    0.271505  Work Trips  Minipilot\n",
       "5     Transit      3    0.008065  Work Trips  Minipilot\n",
       "6        Walk     23    0.061828  Work Trips  Minipilot"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_data #trip breakdown is really close to data used in paper!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ec794",
   "metadata": {},
   "source": [
    "### matching minis to survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "a5b7a264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3492\n",
      "15\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "mini_trips = pd.read_csv('mini_pilot/data/analysis_confirmed_trip.csv')\n",
    "# mini_trips = labeled_mini.copy()\n",
    "mini_surveys = pd.read_csv('mini_pilot/data/survey_household.csv')\n",
    "\n",
    "print(len(mini_trips))\n",
    "print(len(mini_surveys)) #15 surveys\n",
    "print(mini_trips.perno.nunique()) #13 unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "b1ce1036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "socio_data = mini_surveys[~mini_surveys.perno.isnull()]\n",
    "print(len(socio_data))\n",
    "# socio_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "c4eee43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# Deal with people who have multiple responses by using most recent\n",
    "socio_data = socio_data.sort_values(by=['perno', 'timestamp'])\n",
    "socio_data.drop_duplicates(subset=['perno'], keep='last', inplace=True)\n",
    "socio_data['user_id_socio'] = socio_data.perno\n",
    "socio_data.user_id_socio = [i.replace('-','') for i in socio_data.user_id_socio] # remove all dashes from strings\n",
    "socio_data = socio_data.drop(labels='perno', axis=1)\n",
    "\n",
    "print(len(socio_data)) #same as number of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e2253146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "3662\n"
     ]
    }
   ],
   "source": [
    "# Lose some trips due to people with no survey responses\n",
    "mini_trips['user_id_socio'] = mini_trips.perno.astype(str)\n",
    "mini_trips.user_id_socio = [i.replace('-','') for i in mini_trips.user_id_socio] # remove all dashes from strings\n",
    "mini_trips = mini_trips.merge(socio_data, on='user_id_socio')\n",
    "\n",
    "print(mini_trips.user_id_socio.nunique()) #lost one person that has no survey record -- down to 12 people\n",
    "print(len(mini_trips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "d8519ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini_trips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42584730",
   "metadata": {},
   "source": [
    "## Full Pilot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "85cd717a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting with  4c_21\n",
      "10121 trips\n",
      "14 people\n",
      "28 surveys\n",
      "28 surveys after dropping null ids\n",
      "15 surveys 15 users after dropping\n",
      "8874 trips after merging\n",
      "13 people after merging\n",
      "starting with  cc_21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102/3798875609.py:9: DtypeWarning: Columns (5,6,9,10,12,17,24,25,28,29,31,36,42,44,45,47,51,53,54,55,56,57,58) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  trips = pd.read_csv('CEO/ceo_' + program + '-download/data/analysis_confirmed_trip.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75199 trips\n",
      "52 people\n",
      "72 surveys\n",
      "72 surveys after dropping null ids\n",
      "50 surveys 50 users after dropping\n",
      "72275 trips after merging\n",
      "47 people after merging\n",
      "starting with  fc_21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102/3798875609.py:9: DtypeWarning: Columns (3,8,9,10,13,15,19,20,21,24,27,28,32,33,34,37,40,53,54,55,56,57,58) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  trips = pd.read_csv('CEO/ceo_' + program + '-download/data/analysis_confirmed_trip.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32442 trips\n",
      "30 people\n",
      "47 surveys\n",
      "47 surveys after dropping null ids\n",
      "30 surveys 30 users after dropping\n",
      "32341 trips after merging\n",
      "29 people after merging\n",
      "starting with  pc_21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102/3798875609.py:9: DtypeWarning: Columns (3,8,9,10,13,15,19,20,21,24,27,28,32,33,34,37,40,53,54,55,56,57,58) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  trips = pd.read_csv('CEO/ceo_' + program + '-download/data/analysis_confirmed_trip.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17989 trips\n",
      "22 people\n",
      "29 surveys\n",
      "29 surveys after dropping null ids\n",
      "15 surveys 15 users after dropping\n",
      "15565 trips after merging\n",
      "14 people after merging\n",
      "starting with  sc_21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102/3798875609.py:9: DtypeWarning: Columns (3,8,9,10,13,15,19,20,21,24,27,28,32,33,34,37,40,53,54,55,56,57,58) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  trips = pd.read_csv('CEO/ceo_' + program + '-download/data/analysis_confirmed_trip.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17989 trips\n",
      "22 people\n",
      "29 surveys\n",
      "29 surveys after dropping null ids\n",
      "15 surveys 15 users after dropping\n",
      "15565 trips after merging\n",
      "14 people after merging\n",
      "starting with  vail_22\n",
      "9133 trips\n",
      "12 people\n",
      "11 surveys\n",
      "11 surveys after dropping null ids\n",
      "9 surveys 9 users after dropping\n",
      "7447 trips after merging\n",
      "9 people after merging\n"
     ]
    }
   ],
   "source": [
    "#loop over\n",
    "programs = ['4c_21', 'cc_21', 'fc_21', 'pc_21', 'sc_21', 'vail_22']\n",
    "datasets = []\n",
    "\n",
    "for program in programs:\n",
    "    print('starting with ', program)\n",
    "    \n",
    "    #create dataset with surveys and trips\n",
    "    trips = pd.read_csv('CEO/ceo_' + program + '-download/data/analysis_confirmed_trip.csv')\n",
    "    print(len(trips), 'trips')\n",
    "    print(trips.perno.nunique(), 'people')\n",
    "\n",
    "    surveys = pd.read_csv('CEO/ceo_' + program + '-download/data/survey_household.csv')\n",
    "    print(len(surveys), 'surveys')\n",
    "\n",
    "    #drop any null ids\n",
    "    socio_data = surveys[~surveys.unique_user_id_autofilled_do_not_edit.isnull()]\n",
    "    print(len(socio_data), 'surveys after dropping null ids')\n",
    "\n",
    "    #drop duplicates\n",
    "    socio_data = socio_data.sort_values(by=['unique_user_id_autofilled_do_not_edit', 'timestamp'])\n",
    "    socio_data.drop_duplicates(subset=['unique_user_id_autofilled_do_not_edit'], keep='last', inplace=True)\n",
    "    print(len(socio_data),'surveys', socio_data.unique_user_id_autofilled_do_not_edit.nunique(), 'users after dropping')\n",
    "\n",
    "    #prepare survey ids for merging\n",
    "    socio_data['user_id_socio'] = socio_data.unique_user_id_autofilled_do_not_edit.astype(str)\n",
    "    socio_data['user_id_socio'] = socio_data['user_id_socio'].str.strip() #remove leading or trailing whitespace!!\n",
    "    socio_data['user_id_socio'] = socio_data['user_id_socio'] + program\n",
    "    socio_data = socio_data.drop(labels='unique_user_id_autofilled_do_not_edit', axis=1)\n",
    "\n",
    "    #prepare trip ids for merging\n",
    "    trips['user_id_socio'] = trips.perno.astype(str)\n",
    "    trips['user_id_socio'] = trips['user_id_socio'].str.strip() #remove leading or trailing whitespace!!\n",
    "    trips.user_id_socio = [i.replace('-','') for i in trips.user_id_socio] # remove all dashes from strings\n",
    "    trips['user_id_socio'] = trips['user_id_socio'] + program\n",
    "\n",
    "    #merge the data\n",
    "    data = trips.merge(socio_data, on='user_id_socio')\n",
    "    print(len(data), 'trips after merging')\n",
    "    print(data.user_id_socio.nunique(), 'people after merging')\n",
    "    \n",
    "    data['program'] = program.split('_')[0]\n",
    "    \n",
    "    #add to list of datasets\n",
    "    datasets.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b16ae80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152067\n",
      "126\n",
      "65440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['4c', 'cc', 'fc', 'pc', 'sc', 'vail'], dtype=object)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merge them all together\n",
    "full_data = pd.concat(datasets)\n",
    "print(len(full_data)) #152,067 trips\n",
    "print(full_data.user_id_socio.nunique()) #126 users\n",
    "# full_data.columns\n",
    "\n",
    "#filter out unlabeled trips\n",
    "labeled_data = full_data[full_data.data_user_input_mode_confirm.notna() | full_data.data_user_input_purpose_confirm.notna()]\n",
    "# data = data[data.data_user_input_purpose_confirm.notna()]\n",
    "\n",
    "print(len(labeled_data))\n",
    "labeled_data.program.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "9b8e31bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102/1215677822.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labeled_data.rename(columns={'user_id_socio':'user_id',\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, 'REDACTED'], dtype=object)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data.rename(columns={'user_id_socio':'user_id',\n",
    "                          'please_identify_which_category_represents_your_total_household_':'HHINC',\n",
    "                          'how_many_motor_vehicles_are_owned_leased_or_available_for_regul':'VEH',\n",
    "                            ' how_many_motor_vehicles_are_owned_leased_or_available_for_regul':'VEH',\n",
    "                             'how_many_motor_vehicles_are_owned_leased_or_available_for_regul ':'VEH',\n",
    "                           'in_which_year_were_you_born?':'AGE',\n",
    "                          'including_yourself_how_many_people_live_in_your_home?':'HHSIZE',\n",
    "                          'how_many_children_under_age_18_live_in_your_home?':'CHILDREN',\n",
    "                          'what_is_your_gender?':'GENDER',\n",
    "                          'if_you_were_unable_to_use_your_household_vehicles_which_of_the_':'available_modes',\n",
    "                          'are_you_a_student?':'STUDENT',\n",
    "                         'data_duration':'duration', \n",
    "                         'data_distance':'distance'}, inplace=True)\n",
    "\n",
    "labeled_data.AGE.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8366cc1",
   "metadata": {},
   "source": [
    "so far so good, we're looking for at least 122 users and at least 61,496 trips after ALL cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "36d1415d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4c' 'cc' 'fc' 'pc' 'sc' 'vail']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102/3145178333.py:57: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  data.mode_confirm = pd.Categorical(data.data_user_input_mode_confirm, ordered=True, categories=np.unique(list(dic_re.keys())))\n"
     ]
    }
   ],
   "source": [
    "data = labeled_data.copy()\n",
    "\n",
    "#first, add the cleaned mode\n",
    "data['Mode_confirm']= data['data_user_input_mode_confirm'].map(dic_re)\n",
    "\n",
    "#second, add the cleaned replaced mode ASSUMES PROGRAM\n",
    "data['Replaced_mode']= data['data_user_input_replaced_mode'].map(dic_re)\n",
    "\n",
    "#third, add the cleaned purpose\n",
    "data['Trip_purpose']= data['data_user_input_purpose_confirm'].map(dic_pur)\n",
    "\n",
    "# Get timestamp from known year/month/day aggregated to days\n",
    "data.rename(columns={'data_start_local_dt_year':'year','data_start_local_dt_month':'month','data_start_local_dt_day':'day'}, inplace=True)\n",
    "data['date_time'] = pd.to_datetime(data[['year','month','day']])\n",
    "\n",
    "# Fix age (birth year to age)\n",
    "# data['AGE'] = 2022 - data['AGE'] #redacted in data, avaliable after you apply\n",
    "\n",
    "# Number of workers (size of HH - kids)\n",
    "data['WORKERS'] = data['HHSIZE'] - data['CHILDREN']\n",
    "\n",
    "# Duration in minutes (hours to minutes)\n",
    "data['duration'] = data['duration'] / 60\n",
    "\n",
    "# E-bike/not E-Bike variable\n",
    "data['is_ebike'] = \"E-Bike Trips\"\n",
    "data.loc[data['Mode_confirm']!=\"E-bike\", 'is_ebike'] = \"Non E-Bike Trips\"\n",
    "\n",
    "data = data[~data['HHINC'].isin(['Prefer not to say', '$150,000'])] # Side note why is 150k (n=7) its own bin?\n",
    "data['HHINC_NUM'] = data.HHINC.replace(['Less than $24,999',\n",
    "                                       '$25,000-$49,999',\n",
    "                                       '$50,000-$99,999',\n",
    "                                       '$100,000 -$149,999',\n",
    "                                       '$150,000-$199,999',\n",
    "                                       '$200,000 or more'], [12500,37500,75000,125000,175000,250000])\n",
    "\n",
    "# Calculate average income per adult in the household\n",
    "data['PINC'] = data['HHINC_NUM'] / data['WORKERS']\n",
    "\n",
    "# Combine variable categories\n",
    "data = data.replace('Gas Car, drove alone', 'Car')\n",
    "data = data.replace('Gas Car, with others', 'Shared Car')\n",
    "data = data.replace('Bikeshare', 'Shared Micromobility')\n",
    "data = data.replace('Scooter share', 'Shared Micromobility')\n",
    "data = data.replace('Regular Bike', 'Personal Micromobility')\n",
    "data = data.replace('Skate board', 'Personal Micromobility')\n",
    "data = data.replace('Train', 'Transit')\n",
    "data = data.replace('Free Shuttle', 'Transit')\n",
    "data = data.replace('Bus', 'Transit')\n",
    "data = data.replace('Walk', 'Walk')\n",
    "data = data.replace('Taxi/Uber/Lyft', 'Ridehail')\n",
    "data = data.replace('Pilot ebike', 'E-Bike')\n",
    "\n",
    "# Categorical type will include all days/modes in groupby even if there is no data for a particular tabulation\n",
    "data.user_id = pd.Categorical(data.user_id)\n",
    "data.date_time = pd.Categorical(data.date_time)\n",
    "data.mode_confirm = pd.Categorical(data.data_user_input_mode_confirm, ordered=True, categories=np.unique(list(dic_re.keys())))\n",
    "\n",
    "# Add order to categorical variables\n",
    "data.HHINC = pd.Categorical(data.HHINC, ordered=True, categories=['Less than $24,999',\n",
    "                                                                 '$25,000-$49,999',\n",
    "                                                                 '$50,000-$99,999'])\n",
    "data['Mode'] = pd.Categorical(data.Mode_confirm, ordered=True, categories=[\n",
    "    'E-bike',\n",
    "    'Car',\n",
    "    'Shared Car',\n",
    "    'Walk',\n",
    "    'Transit',\n",
    "    'Personal Micromobility',\n",
    "    'Shared Micromobility',\n",
    "    'Ridehail',\n",
    "    'Other'])\n",
    "data.VEH = data.VEH.astype(str)\n",
    "data.VEH = pd.Categorical(data.VEH, ordered=True, categories=['0','1','2','3','4+'])\n",
    "data['PINC_NUM'] = data['PINC']\n",
    "data.PINC = pd.cut(data.PINC, bins=[0,10000,20000,30000,40000,50000,60000,70000,999999],\n",
    "                  labels=[\"$0-9\",\n",
    "                         \"$10-19\",\n",
    "                         \"$20-29\",\n",
    "                         \"$30-39\",\n",
    "                         \"$40-49\",\n",
    "                         \"$50-59\",\n",
    "                         \"$60-69\",\n",
    "                         \"$70+\"])\n",
    "\n",
    "print(data.program.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "fe1aba05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['4c', 'cc', 'fc', 'pc', 'sc', 'vail'], dtype=object)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtered out ages that were greater than 100\n",
    "# data = data[data['AGE'] < 100] #lack data at the moment\n",
    "#filter out durations longer than 8 hours\n",
    "data = data[data['duration']<480]\n",
    "#distances more than 50 miles \n",
    "# data = data[data['distance_miles']<50] #I think the distance I have is not miles, so omitting for right now\n",
    "#records that had ’prefer not to say’ as a response for household income, household vehicles, and other available modes\n",
    "data = data[~data['HHINC'].isin(['Prefer not to say','$100,000 -$149,999','$150,000','$150,000-$199,999','$200,000 or more'])] # Side note why is 150k (n=7) its own bin?\n",
    "data = data[~data['VEH'].isin(['Prefer not to say / Prefiero no decir.'])]\n",
    "data = data[~data['available_modes'].isin(['None', 'Prefer not to say'])]\n",
    "\n",
    "#filter household sizes smaller than the number of kids\n",
    "data = data[data['HHSIZE']>data['CHILDREN']]\n",
    "#filter out households greater than 10\n",
    "data = data[data['HHSIZE']<10]\n",
    "\n",
    "data.program.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "b65998ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59564\n",
      "57606\n"
     ]
    }
   ],
   "source": [
    "# Vehicles per driver\n",
    "data = data[data['VEH'].notna()] #vails VEH nums were not strings?\n",
    "data['VEH_num'] = data['VEH'].replace(['1','2','3','4+'],[1,2,3,4]).astype(int)\n",
    "data['DRIVERS'] = data[\"including_yourself_how_many_people_have_a_driver's_license_in_y\"]\n",
    "data['DRIVERS_num'] = data['DRIVERS'].replace\n",
    "data['veh_per_driver'] = (data['VEH_num'] / data['DRIVERS']).fillna(0)\n",
    "data.loc[data['veh_per_driver']==np.inf, 'veh_per_driver'] = 0\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "#filter out 'not a trip' trips\n",
    "data = data[~data['Mode_confirm'].isin(['Not a Trip'])]\n",
    "data = data[~data['Replaced_mode'].isin(['Not a Trip'])]\n",
    "data = data[~data['Trip_purpose'].isin(['not_a_trip'])]\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "7a0a9c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe split out before first e-bike trip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "be0c9fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['4c', 'cc', 'fc', 'pc', 'sc', 'vail'], dtype=object)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.program.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d9412711",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'data_start_ts':'start_ts'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "738d9e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "43\n",
      "26\n",
      "13\n",
      "13\n",
      "8\n",
      "2560\n",
      "25757\n",
      "11047\n",
      "6930\n",
      "6930\n",
      "4382\n"
     ]
    }
   ],
   "source": [
    "#separating programs\n",
    "four_corners = data[data.program == \"4c\"]\n",
    "community_cycles = data[data.program == \"cc\"]\n",
    "fort_collins = data[data.program == \"fc\"]\n",
    "pueblo = data[data.program == \"pc\"]\n",
    "smart_commute = data[data.program == \"sc\"]\n",
    "vail = data[data.program == \"vail\"]\n",
    "\n",
    "print(four_corners['user_id'].nunique())\n",
    "print(community_cycles['user_id'].nunique())\n",
    "print(fort_collins['user_id'].nunique())\n",
    "print(pueblo['user_id'].nunique())\n",
    "print(smart_commute['user_id'].nunique())\n",
    "print(vail['user_id'].nunique())\n",
    "\n",
    "print(len(four_corners))\n",
    "print(len(community_cycles))\n",
    "print(len(fort_collins))\n",
    "print(len(pueblo))\n",
    "print(len(smart_commute))\n",
    "print(len(vail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "8a24dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering each of them\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52024d4f",
   "metadata": {},
   "source": [
    "# not working, missing timestamps :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d857734d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102/3643144510.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  smart_commute['start_ts']= pd.to_datetime(smart_commute['start_ts'], utc=True, unit='s')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'smart_commute_ebike_first' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[249], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m date \u001b[38;5;129;01min\u001b[39;00m sc_ebike_first[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_ts\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     21\u001b[0m         smart_commute_ebike_first\u001b[38;5;241m=\u001b[39msmart_commute_incl_ebike[(smart_commute_incl_ebike[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_ts\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m date)]\n\u001b[0;32m---> 23\u001b[0m sc_unique_ebikefirst\u001b[38;5;241m=\u001b[39m\u001b[43msmart_commute_ebike_first\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(smart_commute_ebike_first[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()) \u001b[38;5;66;03m#11\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'smart_commute_ebike_first' is not defined"
     ]
    }
   ],
   "source": [
    "#smart commute filtering\n",
    "\n",
    "#timestamp conversion\n",
    "smart_commute['start_ts']= pd.to_datetime(smart_commute['start_ts'], utc=True, unit='s')\n",
    "\n",
    "#grouping, counting unique users\n",
    "trip_sep=smart_commute.groupby(['user_id','Mode_confirm']).apply(lambda x:x[x.start_ts==min(x.start_ts)])\n",
    "print(trip_sep['user_id'].nunique())\n",
    "\n",
    "#consider only trips with E-bike (to get first e-bike trip)\n",
    "sc_ebike_first=trip_sep[trip_sep['Mode_confirm']=='E-bike']\n",
    "\n",
    "#get all the trips by ysers who ever had an e-bike trip\n",
    "sc_ebike_user_list= sc_ebike_first['user_id'].tolist()\n",
    "smart_commute_incl_ebike = smart_commute[smart_commute['user_id'].isin(sc_ebike_user_list)]\n",
    "print(smart_commute_incl_ebike['user_id'].nunique())\n",
    "\n",
    "#filter to the earliest ebike trip\n",
    "for unique_id in sc_ebike_first['user_id']:\n",
    "    for date in sc_ebike_first['start_ts']:\n",
    "        smart_commute_ebike_first=smart_commute_incl_ebike[(smart_commute_incl_ebike['start_ts'] >= date)]\n",
    "\n",
    "sc_unique_ebikefirst=smart_commute_ebike_first['user_id'].unique()\n",
    "print(smart_commute_ebike_first['user_id'].nunique()) #11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "27d8ba70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102/4259660508.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  four_corners['start_ts']= pd.to_datetime(four_corners['start_ts'], utc=True, unit='s')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'four_corners_ebike_first' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[250], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m date \u001b[38;5;129;01min\u001b[39;00m fc_ebike_first[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_ts\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     15\u001b[0m         four_corners_ebike_first\u001b[38;5;241m=\u001b[39mfour_corners_incl_ebike[(four_corners_incl_ebike[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_ts\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m date)]\n\u001b[0;32m---> 17\u001b[0m fc_unique_ebikefirst\u001b[38;5;241m=\u001b[39m\u001b[43mfour_corners_ebike_first\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(four_corners_ebike_first[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'four_corners_ebike_first' is not defined"
     ]
    }
   ],
   "source": [
    "#filter four corners\n",
    "four_corners['start_ts']= pd.to_datetime(four_corners['start_ts'], utc=True, unit='s')\n",
    "\n",
    "trip_sep_fc=four_corners.groupby(['user_id','Mode_confirm']).apply(lambda x:x[x.start_ts==min(x.start_ts)])\n",
    "print(trip_sep_fc['user_id'].nunique())\n",
    "\n",
    "fc_ebike_first=trip_sep_fc[trip_sep_fc['Mode_confirm']=='E-bike']\n",
    "\n",
    "fc_ebike_user_list= fc_ebike_first['user_id'].tolist()\n",
    "four_corners_incl_ebike = four_corners[four_corners['user_id'].isin(fc_ebike_user_list)]\n",
    "print(four_corners_incl_ebike['user_id'].nunique())\n",
    "\n",
    "for unique_id in fc_ebike_first['user_id']:\n",
    "    for date in fc_ebike_first['start_ts']:\n",
    "        four_corners_ebike_first=four_corners_incl_ebike[(four_corners_incl_ebike['start_ts'] >= date)]\n",
    "        \n",
    "fc_unique_ebikefirst=four_corners_ebike_first['user_id'].unique()\n",
    "print(four_corners_ebike_first['user_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "4bacd9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102/1138286727.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  community_cycles['start_ts']= pd.to_datetime(community_cycles['start_ts'], utc=True, unit='s')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'community_cycles_ebike_first' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[251], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m date \u001b[38;5;129;01min\u001b[39;00m cc_ebike_first[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_ts\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     15\u001b[0m         community_cycles_ebike_first\u001b[38;5;241m=\u001b[39mcommunity_cycles_incl_ebike[(community_cycles_incl_ebike[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_ts\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m date)]\n\u001b[0;32m---> 17\u001b[0m cc_unique_ebikefirst\u001b[38;5;241m=\u001b[39m\u001b[43mcommunity_cycles_ebike_first\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(community_cycles_ebike_first[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'community_cycles_ebike_first' is not defined"
     ]
    }
   ],
   "source": [
    "#filtering community cycles\n",
    "community_cycles['start_ts']= pd.to_datetime(community_cycles['start_ts'], utc=True, unit='s')\n",
    "\n",
    "trip_sep_cc=community_cycles.groupby(['user_id','Mode_confirm']).apply(lambda x:x[x.start_ts==min(x.start_ts)])\n",
    "print(trip_sep_cc['user_id'].nunique())\n",
    "\n",
    "cc_ebike_first=trip_sep_cc[trip_sep_cc['Mode_confirm']=='E-bike']\n",
    "\n",
    "cc_ebike_user_list= cc_ebike_first['user_id'].tolist()\n",
    "community_cycles_incl_ebike = community_cycles[community_cycles['user_id'].isin(cc_ebike_user_list)]\n",
    "print(community_cycles_incl_ebike['user_id'].nunique())\n",
    "\n",
    "for unique_id in cc_ebike_first['user_id']:\n",
    "    for date in cc_ebike_first['start_ts']:\n",
    "        community_cycles_ebike_first=community_cycles_incl_ebike[(community_cycles_incl_ebike['start_ts'] >= date)]\n",
    "\n",
    "cc_unique_ebikefirst=community_cycles_ebike_first['user_id'].unique()\n",
    "print(community_cycles_ebike_first['user_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "71e4088b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102/1774079327.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fort_collins['start_ts']= pd.to_datetime(fort_collins['start_ts'], utc=True, unit='s')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fort_collins_ebike_first' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[252], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m date \u001b[38;5;129;01min\u001b[39;00m fc_ebike_first[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_ts\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     15\u001b[0m         fort_collins_ebike_first\u001b[38;5;241m=\u001b[39mfort_collins_incl_ebike[(fort_collins_incl_ebike[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_ts\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m date)]\n\u001b[0;32m---> 17\u001b[0m fc_unique_ebikefirst\u001b[38;5;241m=\u001b[39m\u001b[43mfort_collins_ebike_first\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(fort_collins_ebike_first[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fort_collins_ebike_first' is not defined"
     ]
    }
   ],
   "source": [
    "#filtering fort collins\n",
    "fort_collins['start_ts']= pd.to_datetime(fort_collins['start_ts'], utc=True, unit='s')\n",
    "\n",
    "trip_sep_fc=fort_collins.groupby(['user_id','Mode_confirm']).apply(lambda x:x[x.start_ts==min(x.start_ts)])\n",
    "print(trip_sep_fc['user_id'].nunique())\n",
    "\n",
    "fc_ebike_first=trip_sep_fc[trip_sep_fc['Mode_confirm']=='E-bike']\n",
    "\n",
    "fc_ebike_user_list= fc_ebike_first['user_id'].tolist()\n",
    "fort_collins_incl_ebike = fort_collins[fort_collins['user_id'].isin(fc_ebike_user_list)]\n",
    "print(fort_collins_incl_ebike['user_id'].nunique())\n",
    "\n",
    "for unique_id in fc_ebike_first['user_id']:\n",
    "    for date in fc_ebike_first['start_ts']:\n",
    "        fort_collins_ebike_first=fort_collins_incl_ebike[(fort_collins_incl_ebike['start_ts'] >= date)]\n",
    "        \n",
    "fc_unique_ebikefirst=fort_collins_ebike_first['user_id'].unique()\n",
    "print(fort_collins_ebike_first['user_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "a502e8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102/950012586.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pueblo['start_ts']= pd.to_datetime(pueblo['start_ts'], utc=True, unit='s')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pueblo_ebike_first' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[253], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m date \u001b[38;5;129;01min\u001b[39;00m pu_ebike_first[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_ts\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     15\u001b[0m         pueblo_ebike_first\u001b[38;5;241m=\u001b[39mpueblo_incl_ebike[(pueblo_incl_ebike[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_ts\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m date)]\n\u001b[0;32m---> 17\u001b[0m pu_unique_ebikefirst\u001b[38;5;241m=\u001b[39m\u001b[43mpueblo_ebike_first\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(pueblo_ebike_first[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pueblo_ebike_first' is not defined"
     ]
    }
   ],
   "source": [
    "#filtering pueblo\n",
    "pueblo['start_ts']= pd.to_datetime(pueblo['start_ts'], utc=True, unit='s')\n",
    "\n",
    "trip_sep_pu=pueblo.groupby(['user_id','Mode_confirm']).apply(lambda x:x[x.start_ts==min(x.start_ts)])\n",
    "print(trip_sep_pu['user_id'].nunique())\n",
    "\n",
    "pu_ebike_first=trip_sep_pu[trip_sep_pu['Mode_confirm']=='E-bike']\n",
    "\n",
    "pu_ebike_user_list= pu_ebike_first['user_id'].tolist()\n",
    "pueblo_incl_ebike = pueblo[pueblo['user_id'].isin(pu_ebike_user_list)]\n",
    "print(pueblo_incl_ebike['user_id'].nunique())\n",
    "\n",
    "for unique_id in pu_ebike_first['user_id']:\n",
    "    for date in pu_ebike_first['start_ts']:\n",
    "        pueblo_ebike_first=pueblo_incl_ebike[(pueblo_incl_ebike['start_ts'] >= date)]\n",
    "        \n",
    "pu_unique_ebikefirst=pueblo_ebike_first['user_id'].unique()\n",
    "print(pueblo_ebike_first['user_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "0ed0a787",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non convertible value REDACTED with the unit 's'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda-23.5.2/envs/emission/lib/python3.9/site-packages/pandas/_libs/tslib.pyx:361\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_with_unit_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'REDACTED'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[254], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#filtering vail\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m vail[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_ts\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvail\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstart_ts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m trip_sep_va\u001b[38;5;241m=\u001b[39mvail\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMode_confirm\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x:x[x\u001b[38;5;241m.\u001b[39mstart_ts\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mmin\u001b[39m(x\u001b[38;5;241m.\u001b[39mstart_ts)])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(trip_sep_va[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique())\n",
      "File \u001b[0;32m~/miniconda-23.5.2/envs/emission/lib/python3.9/site-packages/pandas/core/tools/datetimes.py:1064\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1062\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(tz)\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[0;32m-> 1064\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_listlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m   1066\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n",
      "File \u001b[0;32m~/miniconda-23.5.2/envs/emission/lib/python3.9/site-packages/pandas/core/tools/datetimes.py:229\u001b[0m, in \u001b[0;36m_maybe_cache\u001b[0;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[1;32m    227\u001b[0m unique_dates \u001b[38;5;241m=\u001b[39m unique(arg)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[0;32m--> 229\u001b[0m     cache_dates \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda-23.5.2/envs/emission/lib/python3.9/site-packages/pandas/core/tools/datetimes.py:393\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot specify both format and unit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to_datetime_with_unit\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg must be a string, datetime, list, tuple, 1-d array, or Series\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    397\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda-23.5.2/envs/emission/lib/python3.9/site-packages/pandas/core/tools/datetimes.py:557\u001b[0m, in \u001b[0;36m_to_datetime_with_unit\u001b[0;34m(arg, unit, name, tz, errors)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    556\u001b[0m     arg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arg)\n\u001b[0;32m--> 557\u001b[0m     arr, tz_parsed \u001b[38;5;241m=\u001b[39m \u001b[43mtslib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_with_unit_to_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;66;03m# Index constructor _may_ infer to DatetimeIndex\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     result \u001b[38;5;241m=\u001b[39m Index\u001b[38;5;241m.\u001b[39m_with_infer(arr, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/miniconda-23.5.2/envs/emission/lib/python3.9/site-packages/pandas/_libs/tslib.pyx:364\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_with_unit_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: non convertible value REDACTED with the unit 's'"
     ]
    }
   ],
   "source": [
    "#filtering vail\n",
    "vail['start_ts']= pd.to_datetime(vail['start_ts'], utc=True, unit='s')\n",
    "\n",
    "trip_sep_va=vail.groupby(['user_id','Mode_confirm']).apply(lambda x:x[x.start_ts==min(x.start_ts)])\n",
    "print(trip_sep_va['user_id'].nunique())\n",
    "\n",
    "va_ebike_first=trip_sep_va[trip_sep_va['Mode_confirm']=='E-bike']\n",
    "\n",
    "va_ebike_user_list= va_ebike_first['user_id'].tolist()\n",
    "vail_incl_ebike = vail[vail['user_id'].isin(va_ebike_user_list)]\n",
    "print(vail_incl_ebike['user_id'].nunique())\n",
    "\n",
    "for unique_id in va_ebike_first['user_id']:\n",
    "    for date in va_ebike_first['start_ts']:\n",
    "        vail_ebike_first=vail_incl_ebike[(vail_incl_ebike['start_ts'] >= date)]\n",
    "        \n",
    "va_unique_ebikefirst=vail_ebike_first['user_id'].unique()\n",
    "print(vail_ebike_first['user_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "8ab11ffe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'four_corners_ebike_first' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[255], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#checking num users and num trips in each program\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfour_corners_ebike_first\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique())\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(community_cycles_ebike_first[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique())\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(fort_collins_ebike_first[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'four_corners_ebike_first' is not defined"
     ]
    }
   ],
   "source": [
    "#checking num users and num trips in each program\n",
    "print(four_corners_ebike_first['user_id'].nunique())\n",
    "print(community_cycles_ebike_first['user_id'].nunique())\n",
    "print(fort_collins_ebike_first['user_id'].nunique())\n",
    "print(pueblo_ebike_first['user_id'].nunique())\n",
    "print(smart_commute_ebike_first['user_id'].nunique())\n",
    "print(vail_ebike_first['user_id'].nunique())\n",
    "\n",
    "print(len(four_corners_ebike_first))\n",
    "print(len(community_cycles_ebike_first))\n",
    "print(len(fort_collins_ebike_first))\n",
    "print(len(pueblo_ebike_first))\n",
    "print(len(smart_commute_ebike_first))\n",
    "print(len(vail_ebike_first))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "4dc29bd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'four_corners_ebike_first' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[256], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#combining the filtered datasets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m filtered_merged \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mfour_corners_ebike_first\u001b[49m, community_cycles_ebike_first, fort_collins_ebike_first, \n\u001b[1;32m      3\u001b[0m                              pueblo_ebike_first, smart_commute_ebike_first,vail_ebike_first], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(filtered_merged))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(filtered_merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'four_corners_ebike_first' is not defined"
     ]
    }
   ],
   "source": [
    "#combining the filtered datasets\n",
    "filtered_merged = pd.concat([four_corners_ebike_first, community_cycles_ebike_first, fort_collins_ebike_first, \n",
    "                             pueblo_ebike_first, smart_commute_ebike_first,vail_ebike_first], axis=0)\n",
    "print(len(filtered_merged)) #1496\n",
    "print(filtered_merged['user_id'].nunique()) #122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as a csv, to be used as input to analysis!\n",
    "filtered_merged.to_csv(\"tsdc_filtered_merged_trips.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
